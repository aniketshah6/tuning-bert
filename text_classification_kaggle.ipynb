{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096a070-b5d6-4491-a1a0-819fec2f4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following notebook was written to tune a BERT model to classify text\n",
    "#and is only intended to work when executed in a kaggle notebook\n",
    "#the competition is https://www.kaggle.com/competitions/nlp-getting-started\n",
    "\n",
    "#this adapts the approach of the huggingface classify text tutorial (https://huggingface.co/docs/transformers/tasks/sequence_classification)\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#import torch\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install transformers datasets evaluate\n",
    "from transformers import BertModel, AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b978e65-9b4e-4bad-be48-b788bcb1d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data and make a train-test split\n",
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "#rename target to 'label'\n",
    "train = train.rename(columns={'target':'label'})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_t, train_v = train_test_split(train,test_size = 0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68097e76-c313-43bf-ad6c-c1aca70e0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following is preprocessing - it takes pandas dataframes to huggingface datasets\n",
    "#which are converted to tensorflow datasets for tuning the model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "#preproc function\n",
    "def preproc_func(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "def toTok_ds(df):\n",
    "    #it seems that the keyword and location data might have some significance when\n",
    "    #determining whether a tweet is really about a disaster or not, so I put them\n",
    "    #at the start of the text input\n",
    "    df['fullText'] = df['keyword'].astype(str)+' '+df['location'].astype(str)+' '+df['text']\n",
    "    \n",
    "    #drop all columns beside fulltext and rename fulltext to full text\n",
    "    #without this step the dataset is not parsed correctly by the keras metric callback for\n",
    "    #some reason\n",
    "    for col in df.columns:\n",
    "        if col !='fullText' and col !='label':\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    df = df.rename(columns = {'fullText':'text'})\n",
    "    \n",
    "    ds = Dataset.from_pandas(df)\n",
    "    tokenized_ds = ds.map(preproc_func, batched=True)\n",
    "    return tokenized_ds\n",
    "\n",
    "tok_t = toTok_ds(train_t)\n",
    "tok_v = toTok_ds(train_v)\n",
    "\n",
    "tok_t = tok_t.remove_columns([\"__index_level_0__\"])\n",
    "tok_v = tok_v.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "#final tensorflow datasets that get used to tune the model\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tok_t,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tok_v,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3382a62-291c-4b7a-aac4-4abf69e55087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we set up a callback that computes accuracy\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions,axis = 1)\n",
    "    return accuracy.compute(predictions=predictions, references = labels)\n",
    "\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "\n",
    "callbacks = [metric_callback]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b03115d-95a3-4e30-b6b5-093d8e2ab61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, we load the model and tune it with the prepared tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e75f92-d10f-46ab-8e8c-d959ad2c4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"FALSE\", 1: \"TRUE\"}\n",
    "label2id = {\"FALSE\": 0, \"TRUE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc18a7-986e-4b2a-81cb-972e2d717812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tok_t) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a3567-7b9c-46a5-9bb2-55cc1b0726c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0dab05-b7c4-4abc-adff-b84ff15dbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048806a5-4f94-4c2b-9e1a-6521c01187ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5c1e1-d2a7-4b2a-b7aa-143abd458065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f3d58-e438-48c3-8a4d-76c7ce775c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that the model is trained, we want to apply it to the test data to classify as of yet unlabeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322587b-35cf-487f-b2fb-95b7697fe68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fullText'] = test['keyword'].astype(str)+' '+test['location'].astype(str)+' '+test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3ea81-e233-4976-ba3c-0f6b6817fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    output.append(classifier(test['fullText'][i])[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb9024-94bf-4e36-af91-ec93e3ca93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTarget(label):\n",
    "    if label == 'TRUE':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "final_targets = []\n",
    "for label in output:\n",
    "    final_targets.append(toTarget(label))\n",
    "    \n",
    "test['target'] = final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5e6b7-1e9b-47d0-930c-08131bfa76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('output.csv')\n",
    "#note that for a kaggle submission the output will have more columns than it should and an extra header line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
