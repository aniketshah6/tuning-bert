{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f203681-e2be-49ac-ad90-6cbe5e69e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: aiohttp in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2022.7.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.5.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-macosx_10_14_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aniketsmacbookair/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, xxhash, pyarrow, dill, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "Successfully installed datasets-2.11.0 dill-0.3.6 evaluate-0.4.0 huggingface-hub-0.13.3 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 tokenizers-0.13.3 transformers-4.27.4 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "#the following notebook was written to tune a BERT model to classify text\n",
    "#and is only intended to work when executed in a kaggle notebook\n",
    "#the competition is https://www.kaggle.com/competitions/nlp-getting-started\n",
    "\n",
    "#this adapts the approach of the huggingface classify text tutorial (https://huggingface.co/docs/transformers/tasks/sequence_classification)\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#import torch\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install transformers datasets evaluate\n",
    "from transformers import BertModel, AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b978e65-9b4e-4bad-be48-b788bcb1d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data and make a train-test split\n",
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "#rename target to 'label'\n",
    "train = train.rename(columns={'target':'label'})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_t, train_v = train_test_split(train,test_size = 0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68097e76-c313-43bf-ad6c-c1aca70e0b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9427eb5d-507e-4eb9-82cd-a2c20719dd63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sq/znpt64ms0n12r9j6yy87dq_m0000gn/T/ipykernel_5603/87647568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#the following is preprocessing - it takes pandas dataframes to huggingface datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#preproc function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreproc_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#the following is preprocessing - it takes pandas dataframes to huggingface datasets\n",
    "#which are converted to tensorflow datasets for tuning the model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "#preproc function\n",
    "def preproc_func(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "def toTok_ds(df):\n",
    "    #it seems that the keyword and location data might have some significance when\n",
    "    #determining whether a tweet is really about a disaster or not, so I put them\n",
    "    #at the start of the text input\n",
    "    df['fullText'] = df['keyword'].astype(str)+' '+df['location'].astype(str)+' '+df['text']\n",
    "    \n",
    "    #drop all columns beside fulltext and rename fulltext to full text\n",
    "    #without this step the dataset is not parsed correctly by the keras metric callback for\n",
    "    #some reason\n",
    "    for col in df.columns:\n",
    "        if col !='fullText' and col !='label':\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    df = df.rename(columns = {'fullText':'text'})\n",
    "    \n",
    "    ds = Dataset.from_pandas(df)\n",
    "    tokenized_ds = ds.map(preproc_func, batched=True)\n",
    "    return tokenized_ds\n",
    "\n",
    "tok_t = toTok_ds(train_t)\n",
    "tok_v = toTok_ds(train_v)\n",
    "\n",
    "tok_t = tok_t.remove_columns([\"__index_level_0__\"])\n",
    "tok_v = tok_v.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "#final tensorflow datasets that get used to tune the model\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tok_t,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tok_v,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3382a62-291c-4b7a-aac4-4abf69e55087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we set up a callback that computes accuracy\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions,axis = 1)\n",
    "    return accuracy.compute(predictions=predictions, references = labels)\n",
    "\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "\n",
    "callbacks = [metric_callback]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b03115d-95a3-4e30-b6b5-093d8e2ab61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, we load the model and tune it with the prepared tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e75f92-d10f-46ab-8e8c-d959ad2c4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"FALSE\", 1: \"TRUE\"}\n",
    "label2id = {\"FALSE\": 0, \"TRUE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc18a7-986e-4b2a-81cb-972e2d717812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tok_t) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a3567-7b9c-46a5-9bb2-55cc1b0726c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0dab05-b7c4-4abc-adff-b84ff15dbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048806a5-4f94-4c2b-9e1a-6521c01187ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5c1e1-d2a7-4b2a-b7aa-143abd458065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f3d58-e438-48c3-8a4d-76c7ce775c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that the model is trained, we want to apply it to the test data to classify as of yet unlabeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322587b-35cf-487f-b2fb-95b7697fe68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fullText'] = test['keyword'].astype(str)+' '+test['location'].astype(str)+' '+test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3ea81-e233-4976-ba3c-0f6b6817fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    output.append(classifier(test['fullText'][i])[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb9024-94bf-4e36-af91-ec93e3ca93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTarget(label):\n",
    "    if label == 'TRUE':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "final_targets = []\n",
    "for label in output:\n",
    "    final_targets.append(toTarget(label))\n",
    "    \n",
    "test['target'] = final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5e6b7-1e9b-47d0-930c-08131bfa76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('output.csv')\n",
    "#note that for a kaggle submission the output will have more columns than it should and an extra header line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
